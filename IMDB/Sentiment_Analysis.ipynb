{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment Analysis.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8l7IHMrDIgY"
      },
      "source": [
        "import tensorflow_datasets as tfds\r\n",
        "import tensorflow as tf"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cSoY0Kz4FbKX"
      },
      "source": [
        "BUFFER_SIZE = 10000\r\n",
        "BATCH_SIZE = 64\r\n",
        "padded_shapes = ([None], ())"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "koDz3oY0E5Pv",
        "outputId": "a859389c-264d-4b2d-9e6a-41416fe6c6cc"
      },
      "source": [
        "dataset, info = tfds.load('imdb_reviews/subwords8k', with_info=True,\r\n",
        "                          as_supervised=True)\r\n",
        "trainset, testset = dataset['train'], dataset['test']\r\n",
        "encoder = info.features['text'].encoder\r\n",
        "trainset = trainset.shuffle(BUFFER_SIZE).padded_batch(BATCH_SIZE,\r\n",
        "                                         padded_shapes=padded_shapes)\r\n",
        "testset = testset.padded_batch(BATCH_SIZE,\r\n",
        "                               padded_shapes=padded_shapes)\r\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:TFDS datasets with text encoding are deprecated and will be removed in a future version. Instead, you should use the plain text version and tokenize the text using `tensorflow_text` (See: https://www.tensorflow.org/tutorials/tensorflow_text/intro#tfdata_example)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJeHSCsXF2Pu"
      },
      "source": [
        "model = tf.keras.Sequential([tf.keras.layers.Embedding(encoder.vocab_size, 64),\r\n",
        "                             tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\r\n",
        "                             tf.keras.layers.Dense(64, activation='relu'),\r\n",
        "                             tf.keras.layers.Dense(1, activation='sigmoid')\r\n",
        "                             ])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NcEeD8AEHT8e",
        "outputId": "b3823564-561a-4c41-dac8-c6dadbebb8ea"
      },
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(1e-4), loss='binary_crossentropy',\r\n",
        "              metrics=['accuracy'])\r\n",
        "history = model.fit(trainset, epochs=5, validation_data=testset,\r\n",
        "                    validation_steps=30)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "391/391 [==============================] - 53s 112ms/step - loss: 0.6799 - accuracy: 0.5443 - val_loss: 0.5028 - val_accuracy: 0.7406\n",
            "Epoch 2/5\n",
            "391/391 [==============================] - 44s 110ms/step - loss: 0.3743 - accuracy: 0.8431 - val_loss: 0.3582 - val_accuracy: 0.8615\n",
            "Epoch 3/5\n",
            "391/391 [==============================] - 43s 110ms/step - loss: 0.2594 - accuracy: 0.9021 - val_loss: 0.3485 - val_accuracy: 0.8625\n",
            "Epoch 4/5\n",
            "391/391 [==============================] - 43s 109ms/step - loss: 0.2211 - accuracy: 0.9218 - val_loss: 0.3438 - val_accuracy: 0.8672\n",
            "Epoch 5/5\n",
            "391/391 [==============================] - 43s 109ms/step - loss: 0.1869 - accuracy: 0.9354 - val_loss: 0.3742 - val_accuracy: 0.8646\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmjPpLfXH1WM"
      },
      "source": [
        "def pad_to_size(vec, size):\r\n",
        "  zeros = [0]*(size - len(vec))\r\n",
        "  vec.extend(zeros)\r\n",
        "  return vec"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EyUMnIbwH5O3"
      },
      "source": [
        "def sample_predict(sentence, pad, model_):\r\n",
        "  encoded_sample_pred_text = encoder.encode(sentence)\r\n",
        "  if pad:\r\n",
        "    encoded_sample_pred_text = pad_to_size(encoded_sample_pred_text, 64)\r\n",
        "\r\n",
        "  encoded_sample_pred_text = tf.cast(encoded_sample_pred_text, tf.float32)\r\n",
        "  predictions = model_.predict(tf.expand_dims(encoded_sample_pred_text, 0))\r\n",
        "\r\n",
        "  return predictions"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evfnHyllJnqU",
        "outputId": "3b3c7fca-bf33-4241-f04a-89e11e7f14e3"
      },
      "source": [
        "# Dummy Reviews\r\n",
        "sample_text_pos = ('This movie was so awesome. Acting was really nice, I recommend it')\r\n",
        "predictions1 = sample_predict(sample_text_pos, pad=True, model_=model) * 100\r\n",
        "print('Probability of Positive review %.2f' % predictions1)\r\n",
        "sample_text_neg = ('I did not like the movie. The acting was okayish')\r\n",
        "predictions2 = sample_predict(sample_text_neg, pad=True, model_=model) * 100\r\n",
        "print('Probability of Positive review %.2f' % predictions2)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Probability of Positive review 62.91\n",
            "Probability of Positive review 26.84\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJ4YCJenLUTU"
      },
      "source": [
        "Now let's make a more complex model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qM6gw2EILF4u"
      },
      "source": [
        "model2 = tf.keras.Sequential([tf.keras.layers.Embedding(encoder.vocab_size, 64),\r\n",
        "                              tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\r\n",
        "                              tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\r\n",
        "                              tf.keras.layers.Dense(64, activation='relu'),\r\n",
        "                              tf.keras.layers.Dropout(0.5),\r\n",
        "                              tf.keras.layers.Dense(1, activation='sigmoid')\r\n",
        "                              ])"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0zKzdGbMqHC",
        "outputId": "6ebdab24-ae5e-40ee-8369-b3f974da7f5d"
      },
      "source": [
        "model2.compile(optimizer=tf.keras.optimizers.Adam(1e-4), loss='binary_crossentropy',\r\n",
        "              metrics=['accuracy'])\r\n",
        "history2 = model2.fit(trainset, epochs=5, validation_data=testset,\r\n",
        "                    validation_steps=30)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "391/391 [==============================] - 81s 194ms/step - loss: 0.6757 - accuracy: 0.5407 - val_loss: 0.4345 - val_accuracy: 0.8073\n",
            "Epoch 2/5\n",
            "391/391 [==============================] - 75s 189ms/step - loss: 0.3594 - accuracy: 0.8572 - val_loss: 0.3711 - val_accuracy: 0.8490\n",
            "Epoch 3/5\n",
            "391/391 [==============================] - 74s 187ms/step - loss: 0.2635 - accuracy: 0.9057 - val_loss: 0.3619 - val_accuracy: 0.8521\n",
            "Epoch 4/5\n",
            "391/391 [==============================] - 73s 186ms/step - loss: 0.2122 - accuracy: 0.9280 - val_loss: 0.3917 - val_accuracy: 0.8630\n",
            "Epoch 5/5\n",
            "391/391 [==============================] - 74s 188ms/step - loss: 0.1835 - accuracy: 0.9407 - val_loss: 0.3993 - val_accuracy: 0.8599\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFi58dZKM3gF",
        "outputId": "c5fe909b-1236-40fc-dde4-cc98cca8611f"
      },
      "source": [
        "# Dummy Reviews\r\n",
        "sample_text_pos = ('This movie was so awesome. Acting was really nice, I recommend it')\r\n",
        "predictions1 = sample_predict(sample_text_pos, pad=True, model_=model2) * 100\r\n",
        "print('Probability of Positive review %.2f' % predictions1)\r\n",
        "sample_text_neg = ('I did not like the movie. The acting was okayish')\r\n",
        "predictions2 = sample_predict(sample_text_neg, pad=True, model_=model2) * 100\r\n",
        "print('Probability of Positive review %.2f' % predictions2)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Probability of Positive review 88.10\n",
            "Probability of Positive review 23.42\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4MTn1hAPKRi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}